1. Look up the specs for the totient nodes. Having read the Roofline paper,
   draw a roofline diagram for one totient node (assuming only the
   host cores are used, for the moment).  How do things change with
   the addition of the two Phi boards?

Intel Xeon E5-2620 v3
  * Peak FP performance: 346 GFLOPS/s
    - flops_per_cycle = num_cores * ((simd_width/64) * 2 + num_scalar_fpus)
                      = 12 * ((256/64) * 2 + 1) = 108 FLOPS/cycle
    - flops_per_second = flops_per_cycle * cycles_per_second
                       = 108 * 3.2e9 = 345.6 GFLOPS/second
    - Assume single FPU in scalar pipeline and FMAs are supported in the
      SIMD unit, and that all FP operations can be executed
      simultaneously at turbo boost frequency
  * Peak memory bandwidth: 50 GB/s

Intel Xeon Phi Coprocessor 5110P
  * Peak FP performance: 1.13 TFLOPS/s
    - flops_per_cycle = 60 * ((512/64) * 2 + 2) = 1.08 kFLOPS/cycle
    - flops_per_second = 1.08e3 * 1.05e9 = 1.13 TFLOPS/second
    - Assume a scalar FPU in each of the scalar execution pipelines and
      FMAs are supported in the SIMD unit
  * Peak memory bandwidth: 320 GB/s

Adding two Phi boards to a single node will increase the peak performance
by 2 x 1.13 TFLOPS/s = 2.26 TFLOPS/s, as well as increase peak memory
bandwidth by 2 x 320 GB/s = 640 GB/s. At this rate, the Phi boards only
need an operational intensity of around 4 FLOPS/B to achieve peak
performance. However, the Xeon still needs an operational intensity of
around 8 FLOPS/B to achieve peak performance. This means that the total
performance of the system 346 GFLOPS/s + 2.26 TFLOPS/s = 2.61 TFLOPS/s
can only be achieved at an operational intensity of 8 FLOPS/B. At half
the operational intensity, we will still have peak performance from the
Phi boards, but the Xeon will be at half of the peak performance of 173
GFLOPS/s. This difference is quite minor compared to the bulk of the
performance generated by the Phi boards. As such, the roofline model
would have a steep slope at the lower operational intensities, followed
by a much more gradual slope between 4 and 8 FLOPS/B, then a plateau at
the peak performance.

2. What is the difference between two cores and one core with
   hyperthreading?

Hyperthreading is another term for a technique called simultaneous
multithreading, in which multiple threads share the resources of a single
core to exploit parallelism. On the other hand, a system with two
physical cores presumably runs a single thread per core with no need to
share resources between the two cores (aside from shared cache levels).

3. Do a Google search to find a picture of how memories are arranged
   on the Phi architecture.  Describe the setup briefly in your own
   words.  Is the memory access uniform or non-uniform?

Both the L2 caches and the DMAs are connected via a NoC with ring
topology. This is a NUMA system since the communication latency increases
as the distance between nodes on the NoC increases.

4. Consider the parallel dot product implementations suggested in the
   slides.  As a function of the number of processors, the size of the
   vectors, and typical time to send a message, can you predict the
   speedup associated with parallelizing a dot product computation?
   [Note that dot products have low arithmetic intensity -- the
    roofline model may be useful for reasoning about the peak
    performance for computing pieces of the dot product]

num_fp_fmas_per_proc = ceil(num_elements / num_proc)
reduction_time = add_time + (num_proc - 1)
total_parallel_time =
  fma_time * num_fp_fmas_per_proc + message_time + reduction_time

total_serial_time = fma_time * num_elements

rough_speedup = total_serial_time / total_parallel_time

The ideal speedup of factor of num_proc will be limited by the overhead
from inter-core communication for clearing the barrier and the serialized
reduction of the partial sums by the master core.
