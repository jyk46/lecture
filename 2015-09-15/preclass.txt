For the questions regarding the Game of Life, you may want to refer
to the simple implementation included in the "life" subdirectory.
If you run "make glider", you can see a small example of running
the glider pattern for a few generations.

0.  How much time did you spend on this pre-class exercise, and when?

One hour; night before class.

1.  What are one or two points that you found least clear in the
    9/15 slide decks (including the narration)?

On slide 2, the concept of "big blocks with low surface-to-volume ratio"
is unclear in the context of locality.

2.  In the basic implementation provided, what size board for the Game
    of Life would fit in L3 cache for one of the totient nodes?  Add a
    timer to the code and run on the totient node.  How many cells per
    second can we update for a board that fits in L3 cache?  For a
    board that does not fit?

L3 cache is 20MB and each element in the board is 1B (char), so a board
size of 20M (i.e., 20*2^20) elements would completely fill the L3
cache. Biggest practical NxN board would be N=4k.

3.  Assuming that we want to advance time by several generations,
    suggest a blocking strategy that would improve the operational
    intensity of the basic implementation.  Assume the board is
    not dilute, so that we must still consider every cell.  You may
    want to try your hand at implementing your strategy (though you
    need not spend too much time on it).

In order to improve operational intensity, we want to use the data loaded
from DRAM as much as possible before writing it back or eventually
reading it back if we need access to it later.

A naive blocking strategy would attempt to fit both the previous and
current boards into the L1 cache for a given block. All the elements in
the previous board would be used to generate the elements in the current
board, then all the elements in the current board would be used to
generate the elements in the next board (which would overwrite the
previous board's allocated memory), etc. until the number of desired
generations have been advanced. In this way, we would avoid having to
write back to DRAM (or reading back from DRAM) between generations.

However, this ignores the issue of communicating boundary cell
information between blocks. To address this, we could increase the
"radius" of the block by the number of generations we want to compute at
once. This way, each block would only need to read the cells in the block
once and have all the information we need to compute the necessary
generation advancements. One thing to keep in mind is that this buffer
ring around the block (representing the output board) would overlap with
neighboring blocks so that we would have to do redundant computation for
the cells in the buffer ring for each block.

4.  Comment on what would be required to parallelize this code
    according to the domain decomposition strategy outlined in the
    slides.  Do you think you would see good speedups on one of
    the totient nodes?  Why or why not?

A naive parallelization strategy would be to map each domain to a
separate thread and load the area of the domain plus a one element
padding so that each thread has all the information it needs to calculate
the next generation for its domain. We would need to synchronize the
threads with a barrier after every step (or some number of steps, using
the algorithm above) in order to only swap the previous and current
boards once the current board has been completely filled.

As long as there is decent natural load balancing between the domains and
the overhead of the barrier is not too high, we should see reasonable
speedups on the totient node.

5.  Suppose we want to compute long-range interactions between two
    sets of particles in parallel using the obvious n^2 algorithm in a
    shared-memory setting.  A naive implementation might look like

      struct particle_t {
          double x, y;
          double fx, fy;
      };

      // Update p1 with forces from interaction with p2
      void apply_forces(particle* p1, particle* p2);

      // Assume p is the index of the current processor,
      // part_idx[p] <= i < part_idx[p+1] is the range of
      // particles "owned" by processor p.
      //
      for (int i = part_idx[p]; i < part_idx[p+1]; ++i)
          for (int j = 0; j < npart; ++j)
              apply_forces(particle + i, particle + j);

    Based on what you know about memories and parallel architecture,
    do you see any features of this approach that are likely to lead
    to poor performance?

If this kernel runs on two separate proceassors with private L1 caches,
there could be issues with heavy invalidation traffic with the coherence
protocol, since one thread is constantly modifying values that the other
thread needs to read.

In addition, if the number of particles is large enough that it does not
fit in the LLC, we could end up with thrashing where the particles owned
by one thread evicts the particles owned by the other thread, and vice
versa.
